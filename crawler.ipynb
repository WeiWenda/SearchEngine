{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import *\n",
    "from urlparse import urljoin\n",
    "import MySQLdb as mysql\n",
    "import jieba\n",
    "import re\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')   \n",
    "ignorewords=set(['the','of','to','and','a','in','is','it'])\n",
    "class crawler:\n",
    "    def __init__(self,dbname):\n",
    "        self.con=mysql.connect(host=\"localhost\",user=\"root\",passwd=\"root\",db=dbname,charset='utf8')\n",
    "        self.cursor=self.con.cursor()\n",
    "    def __del__(self):\n",
    "        self.cursor.close()\n",
    "        self.con.close()\n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "    def createindextables(self):\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `link` (`rowid` decimal(20,0) NOT NULL DEFAULT '0',`fromid` decimal(20,0) DEFAULT NULL,`toid` decimal(20,0) DEFAULT NULL,PRIMARY KEY (`rowid`),KEY `urltoidx` (`toid`), KEY `urlfromidx` (`fromid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `linkwords` (`wordid` decimal(20,0) NOT NULL DEFAULT '0', `linkid` decimal(20,0) NOT NULL DEFAULT '0', PRIMARY KEY (`wordid`,`linkid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `wordlist` (`word` varchar(50) DEFAULT NULL, `rowid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`rowid`), KEY `wordidx` (`word`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `urllist` (`url` varchar(200) DEFAULT NULL, `rowid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`rowid`), KEY `urlidx` (`url`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `wordlocation` (`location` varchar(50) NOT NULL DEFAULT '', `wordid` decimal(20,0) NOT NULL DEFAULT '0', `urlid` decimal(20,0) NOT NULL DEFAULT '0', PRIMARY KEY (`wordid`,`urlid`,`location`), KEY `wordurlidx` (`wordid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.dbcommit()\n",
    "    # Index an individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        if self.isindexed(url): return\n",
    "        print 'Indexing '+url\n",
    "        # Get the individual words\n",
    "        text=self.gettextonly(soup)\n",
    "        words=self.separatewords(text)\n",
    "        # Get the URL id\n",
    "        urlid=self.getentryid('urllist','url',url)\n",
    "        # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: continue\n",
    "            if len(word)>50: continue\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            self.cursor.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "    def isindexed(self,url):\n",
    "        #there are some wrong\n",
    "        u=self.cursor.execute(\"select rowid from urllist where url='%s'\" % url)\n",
    "        if u>0:\n",
    "            # Check if it has actually been crawled\n",
    "            v=self.cursor.execute('select * from wordlocation where urlid=%d' % self.cursor.fetchone()[0])\n",
    "            if v>0: return True\n",
    "        return False\n",
    "    def getentryid(self,table,field,value,createnew=True):\n",
    "        cur=self.cursor.execute(\"select rowid from %s where %s='%s'\" % (table,field,value))\n",
    "        if cur==0:\n",
    "            self.cursor.execute(\"insert into %s (%s) values ('%s')\" % (table,field,value))\n",
    "            return self.getentryid(table,field,value)\n",
    "        else:\n",
    "            return self.cursor.fetchone()[0]\n",
    "    def gettextonly(self,soup):\n",
    "        v=soup.string\n",
    "        if v==None:\n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=subtext+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip( )\n",
    "    def separatewords(self,text):\n",
    "        return list(jieba.cut_for_search(re.sub(u'([^\\u4e00-\\u9fa5]+)',\"\",text.decode('utf8'))))\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        pass\n",
    "    def crawl(self,pages,depth=1):\n",
    "        for i in range(depth):\n",
    "            newpages=set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue\n",
    "                soup=BeautifulSoup(c.read( ),\"lxml\")\n",
    "                self.addtoindex(page,soup)\n",
    "                links=soup('a')\n",
    "                for link in links:\n",
    "                    if('href' in dict(link.attrs)):\n",
    "                        url=urljoin(page,link['href'])\n",
    "                        if url.find(\"'\")!=-1: continue\t\n",
    "                        url=url.split('#')[0] # remove location portion\n",
    "                        if url[0:4]=='http' and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                        linkText=self.gettextonly(link)\n",
    "                        self.addlinkref(page,url,linkText)\n",
    "                self.dbcommit( )\n",
    "            pages=newpages\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mycrawler=crawler('searchEngine')\n",
    "mycrawler.createindextables()\n",
    "mycrawler.dbcommit()\n",
    "mycrawler.crawl(['http://www.gowhich.com/blog/147?utm_source=tuicool&utm_medium=referral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "[(u'100', 13L, 1L), (u'1001', 13L, 1L), (u'1059', 13L, 1L), (u'1063', 13L, 1L), (u'1067', 13L, 1L), (u'1068', 13L, 1L), (u'1082', 13L, 1L), (u'1088', 13L, 1L), (u'1105', 13L, 1L), (u'12', 13L, 1L), (u'127', 13L, 1L), (u'156', 13L, 1L), (u'163', 13L, 1L), (u'165', 13L, 1L), (u'178', 13L, 1L), (u'191', 13L, 1L), (u'207', 13L, 1L), (u'221', 13L, 1L), (u'241', 13L, 1L), (u'257', 13L, 1L), (u'271', 13L, 1L), (u'279', 13L, 1L), (u'28', 13L, 1L), (u'291', 13L, 1L), (u'298', 13L, 1L), (u'307', 13L, 1L), (u'461', 13L, 1L), (u'468', 13L, 1L), (u'484', 13L, 1L), (u'49', 13L, 1L), (u'493', 13L, 1L), (u'551', 13L, 1L), (u'560', 13L, 1L), (u'574', 13L, 1L), (u'583', 13L, 1L), (u'595', 13L, 1L), (u'613', 13L, 1L), (u'636', 13L, 1L), (u'643', 13L, 1L), (u'663', 13L, 1L), (u'668', 13L, 1L), (u'709', 13L, 1L), (u'714', 13L, 1L), (u'751', 13L, 1L), (u'757', 13L, 1L), (u'765', 13L, 1L), (u'77', 13L, 1L), (u'772', 13L, 1L), (u'779', 13L, 1L), (u'790', 13L, 1L), (u'80', 13L, 1L), (u'802', 13L, 1L), (u'828', 13L, 1L), (u'835', 13L, 1L), (u'837', 13L, 1L), (u'849', 13L, 1L), (u'858', 13L, 1L), (u'897', 13L, 1L), (u'946', 13L, 1L), (u'974', 13L, 1L), (u'977', 13L, 1L)]\n"
     ]
    }
   ],
   "source": [
    "print mycrawler.cursor.execute('select * from wordlocation where wordid=13')\n",
    "print [row for row in mycrawler.cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
