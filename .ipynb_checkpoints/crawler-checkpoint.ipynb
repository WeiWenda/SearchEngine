{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import *\n",
    "from urlparse import urljoin\n",
    "import MySQLdb as mysql\n",
    "import jieba\n",
    "import re\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')   \n",
    "ignorewords=set(['the','of','to','and','a','in','is','it'])\n",
    "class crawler:\n",
    "    def __init__(self,dbname):\n",
    "        self.con=mysql.connect(host=\"localhost\",user=\"root\",passwd=\"root\",db=dbname,charset='utf8')\n",
    "        self.cursor=self.con.cursor()\n",
    "    def __del__(self):\n",
    "        self.cursor.close()\n",
    "        self.con.close()\n",
    "    def dbcommit(self):\n",
    "        self.con.commit()\n",
    "    def createindextables(self):\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `link` (`rowid` bigint(20) NOT NULL AUTO_INCREMENT, `fromid` bigint(20) DEFAULT NULL, `toid` bigint(20) DEFAULT NULL, PRIMARY KEY (`rowid`), KEY `urltoidx` (`toid`), KEY `urlfromidx` (`fromid`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `linkwords` (`wordid` bigint(20) NOT NULL DEFAULT '0', `linkid` bigint(20) NOT NULL DEFAULT '0') ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `wordlist` (`word` varchar(50) DEFAULT NULL, `rowid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`rowid`), KEY `wordidx` (`word`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `urllist` (`url` varchar(200) DEFAULT NULL, `rowid` bigint(20) NOT NULL AUTO_INCREMENT, PRIMARY KEY (`rowid`), KEY `urlidx` (`url`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.cursor.execute(\"CREATE TABLE IF NOT EXISTS `wordlocation` (`location` varchar(50) NOT NULL DEFAULT '', `wordid` bigint(20) NOT NULL DEFAULT '0', `urlid` bigint(20) NOT NULL DEFAULT '0', PRIMARY KEY (`wordid`,`urlid`,`location`), KEY `wordurlidx` (`wordid`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8\")\n",
    "        self.dbcommit()\n",
    "    # Index an individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        if self.isindexed(url): return\n",
    "        print 'Indexing '+url\n",
    "        # Get the individual words\n",
    "        text=self.gettextonly(soup)\n",
    "        words=self.separatewords(text)\n",
    "        # Get the URL id\n",
    "        urlid=self.getentryid('urllist','url',url)\n",
    "        # Link each word to this url\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: continue\n",
    "            if len(word)>50: continue\n",
    "            wordid=self.getentryid('wordlist','word',word)\n",
    "            self.cursor.execute(\"insert into wordlocation(urlid,wordid,location) values (%d,%d,%d)\" % (urlid,wordid,i))\n",
    "    def isindexed(self,url):\n",
    "        #there are some wrong\n",
    "        u=self.cursor.execute(\"select rowid from urllist where url='%s'\" % url)\n",
    "        if u>0:\n",
    "            # Check if it has actually been crawled\n",
    "            v=self.cursor.execute('select * from wordlocation where urlid=%d' % self.cursor.fetchone()[0])\n",
    "            if v>0: return True\n",
    "        return False\n",
    "    def getentryid(self,table,field,value,createnew=True):\n",
    "        cur=self.cursor.execute(\"select rowid from %s where %s='%s'\" % (table,field,value))\n",
    "        if cur==0:\n",
    "            self.cursor.execute(\"insert into %s (%s) values ('%s')\" % (table,field,value))\n",
    "            return self.getentryid(table,field,value)\n",
    "        else:\n",
    "            return self.cursor.fetchone()[0]\n",
    "    def gettextonly(self,soup):\n",
    "        v=soup.string\n",
    "        if v==None:\n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=subtext+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip( )\n",
    "    def separatewords(self,text):\n",
    "        return list(jieba.cut_for_search(re.sub(u'([^\\u4e00-\\u9fa5]+)',\"\",text.decode('utf8'))))\n",
    "    def addlinkref(self,urlFrom,urlTo,linkText):\n",
    "        if len(urlTo)>200:\n",
    "            return\n",
    "        a=int(self.getentryid('urllist','url',urlFrom))\n",
    "        b=int(self.getentryid('urllist','url',urlTo))\n",
    "        self.cursor.execute(\"insert into link (fromid,toid) values (%d,%d)\" % (a,b))\n",
    "        self.cursor.execute(\"select rowid from link where fromid=%d and toid=%d\" % (a,b))\n",
    "        c=int(self.cursor.fetchone()[0])\n",
    "        words=self.separatewords(linkText)\n",
    "        for i in range(len(words)):\n",
    "            word=words[i]\n",
    "            if word in ignorewords: continue\n",
    "            if len(word)>50: continue\n",
    "            wordid=int(self.getentryid('wordlist','word',word))\n",
    "            if self.cursor.execute(\"select * from linkwords where wordid=%d and linkid=%d \" % (wordid,c))==0:\n",
    "                self.cursor.execute(\"insert into linkwords(wordid,linkid) values (%d,%d)\" % (wordid,c))\n",
    "    def crawl(self,pages,depth=1):\n",
    "        for i in range(depth):\n",
    "            newpages=set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib2.urlopen(page)\n",
    "                except:\n",
    "                    print \"Could not open %s\" % page\n",
    "                    continue\n",
    "                soup=BeautifulSoup(c.read( ),\"lxml\")\n",
    "                self.addtoindex(page,soup)\n",
    "                links=soup('a')\n",
    "                for link in links:\n",
    "                    if('href' in dict(link.attrs)):\n",
    "                        url=urljoin(page,link['href'])\n",
    "                        if url.find(\"'\")!=-1: continue\t\n",
    "                        url=url.split('#')[0] # remove location portion\n",
    "                        if url[0:4]=='http' and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                            linkText=self.gettextonly(link)\n",
    "                            self.addlinkref(page,url,linkText)\n",
    "                self.dbcommit( )\n",
    "            pages=newpages\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mycrawler=crawler('searchEngine')\n",
    "mycrawler.createindextables()\n",
    "mycrawler.dbcommit()\n",
    "mycrawler.crawl(['http://www.gowhich.com/blog/147?utm_source=tuicool&utm_medium=referral'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print mycrawler.cursor.execute('select * from wordlocation where wordid=13')\n",
    "print [row for row in mycrawler.cursor.fetchall()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
